{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pprint\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from past.builtins import xrange\n",
    "\n",
    "class MemN2N(object):\n",
    "    def __init__(self, config, sess):\n",
    "        self.nwords = config.nwords\n",
    "        self.init_hid = config.init_hid\n",
    "        self.init_std = config.init_std\n",
    "        self.batch_size = config.batch_size\n",
    "        self.nepoch = config.nepoch\n",
    "        self.nhop = config.nhop\n",
    "        self.edim = config.edim\n",
    "        self.mem_size = config.mem_size\n",
    "        self.lindim = config.lindim\n",
    "        self.max_grad_norm = config.max_grad_norm\n",
    "        self.max_sen_len = 251\n",
    "        \n",
    "        self.show = config.show\n",
    "        self.is_test = config.is_test\n",
    "        self.checkpoint_dir = config.checkpoint_dir\n",
    "\n",
    "        if not os.path.isdir(self.checkpoint_dir):\n",
    "            raise Exception(\" [!] Directory %s not found\" % self.checkpoint_dir)\n",
    "\n",
    "        #self.input = tf.placeholder(tf.float32, [None, self.edim], name=\"input\")\n",
    "        self.time = tf.placeholder(tf.int32, [None, self.mem_size], name=\"time\")\n",
    "        self.target = tf.placeholder(tf.float32, [self.batch_size, self.nwords], name=\"target\")\n",
    "        self.context = tf.placeholder(tf.int32, [self.batch_size, self.mem_size], name=\"context\")\n",
    "        self.query = tf.placeholder(tf.float32 , [self.batch_size , self.max_sen_len] , name='query')\n",
    "        \n",
    "        \n",
    "        self.hid = []\n",
    "        #self.hid.append(self.input)\n",
    "        self.share_list = []\n",
    "        self.share_list.append([])\n",
    "\n",
    "        self.lr = None\n",
    "        self.current_lr = config.init_lr\n",
    "        self.loss = None\n",
    "        self.step = None\n",
    "        self.optim = None\n",
    "\n",
    "        self.sess = sess\n",
    "        self.log_loss = []\n",
    "        self.log_perp = []\n",
    "\n",
    "    def build_memory(self):\n",
    "        self.global_step = tf.Variable(0, name=\"global_step\")\n",
    "\n",
    "        self.A = tf.Variable(tf.random_normal([self.nwords, self.edim], stddev=self.init_std))\n",
    "        self.B = tf.Variable(tf.random_normal([self.nwords, self.edim], stddev=self.init_std))\n",
    "        \n",
    "        #self.embC =  tf.Variable(tf.random_normal([self.nwords, self.edim], stddev=self.init_std))\n",
    "        \n",
    "        self.C = tf.Variable(tf.random_normal([self.edim, self.edim], stddev=self.init_std))\n",
    "\n",
    "        # Temporal Encoding\n",
    "        self.T_A = tf.Variable(tf.random_normal([self.mem_size, self.edim], stddev=self.init_std))\n",
    "        self.T_B = tf.Variable(tf.random_normal([self.mem_size, self.edim], stddev=self.init_std))\n",
    "        self.T_C = tf.Variable(tf.random_normal([self.mem_size, self.edim], stddev=self.init_std))\n",
    "        # m_i = sum A_ij * x_ij + T_A_i\n",
    "        \n",
    "        Ain_c = tf.nn.embedding_lookup(self.A, self.context)\n",
    "        Ain_t = tf.nn.embedding_lookup(self.T_A, self.time)\n",
    "        Ain = tf.add(Ain_c, Ain_t) #m_i\n",
    "\n",
    "        # c_i = sum C_ij * u + T_C_i\n",
    "        Cin_c = tf.nn.embedding_lookup(self.C, self.context)\n",
    "        Cin_t = tf.nn.embedding_lookup(self.T_C, self.time)\n",
    "        Cin = tf.add(Cin_c, Cin_t) #u_i\n",
    "        \n",
    "        # query embedding by B\n",
    "        Bin = tf.nn.embedding_lookup(self.B , self.query)\n",
    "        \n",
    "        #for h in range(self.nhop):\n",
    "        \n",
    "        tmp = tf.matmul(Ain,Cin)\n",
    "        \n",
    "        P = tf.nn.softmax(tmp)\n",
    "        P = tf.reshape(P , [-1,1,self.mem_size])\n",
    "        \n",
    "        O = tf.matmul(P , Cin)\n",
    "        \n",
    "        \n",
    "        self.hid.append(O)\n",
    "\n",
    "    def build_model(self):\n",
    "        self.build_memory()\n",
    "\n",
    "        self.W = tf.Variable(tf.random_normal([self.edim, self.nwords], stddev=self.init_std))\n",
    "        z = tf.matmul(self.hid[-1], self.W)\n",
    "\n",
    "        self.loss = tf.nn.softmax_cross_entropy_with_logits(logits=z, labels=self.target)\n",
    "\n",
    "        self.lr = tf.Variable(self.current_lr)\n",
    "        self.opt = tf.train.GradientDescentOptimizer(self.lr)\n",
    "\n",
    "        params = [self.A, self.B, self.C, self.T_A, self.T_B, self.W]\n",
    "        grads_and_vars = self.opt.compute_gradients(self.loss,params)\n",
    "        clipped_grads_and_vars = [(tf.clip_by_norm(gv[0], self.max_grad_norm), gv[1]) \\\n",
    "                                   for gv in grads_and_vars]\n",
    "\n",
    "        inc = self.global_step.assign_add(1)\n",
    "        with tf.control_dependencies([inc]):\n",
    "            self.optim = self.opt.apply_gradients(clipped_grads_and_vars)\n",
    "\n",
    "        tf.global_variables_initializer().run()\n",
    "        self.saver = tf.train.Saver()\n",
    "\n",
    "    def train(self, data):\n",
    "        N = int(math.ceil(len(data) / self.batch_size))\n",
    "        cost = 0\n",
    "        \n",
    "        query = np.ndarray([self.batch_size , self.max_sen_len] , dtype = np.float32)\n",
    "        x = np.ndarray([self.batch_size, self.edim], dtype=np.float32)\n",
    "        time = np.ndarray([self.batch_size, self.mem_size], dtype=np.int32)\n",
    "        target = np.zeros([self.batch_size, self.nwords]) # one-hot-encoded\n",
    "        context = np.ndarray([self.batch_size, self.mem_size])\n",
    "\n",
    "        x.fill(self.init_hid)\n",
    "        \n",
    "        for t in xrange(self.mem_size):\n",
    "            time[:,t].fill(t)\n",
    "\n",
    "        if self.show:\n",
    "            from utils import ProgressBar\n",
    "            bar = ProgressBar('Train', max=N)\n",
    "\n",
    "        for idx in xrange(N):\n",
    "            if self.show: bar.next()\n",
    "            target.fill(0)\n",
    "            for b in xrange(self.batch_size):\n",
    "                m = random.randrange(self.mem_size, len(data))\n",
    "                target[b][data[m]] = 1\n",
    "                context[b] = data[m - self.mem_size:m]\n",
    "\n",
    "            _, loss, self.step = self.sess.run([self.optim,\n",
    "                                                self.loss,\n",
    "                                                self.global_step],\n",
    "                                                feed_dict={\n",
    "                                                    self.query: query,\n",
    "                                                    self.time: time,\n",
    "                                                    self.target: target,\n",
    "                                                    self.context: context})\n",
    "            cost += np.sum(loss)\n",
    "\n",
    "        if self.show: bar.finish()\n",
    "        return cost/N/self.batch_size\n",
    "\n",
    "    def test(self, data, label='Test'):\n",
    "        N = int(math.ceil(len(data) / self.batch_size))\n",
    "        cost = 0\n",
    "\n",
    "        x = np.ndarray([self.batch_size, self.edim], dtype=np.float32)\n",
    "        time = np.ndarray([self.batch_size, self.mem_size], dtype=np.int32)\n",
    "        target = np.zeros([self.batch_size, self.nwords]) # one-hot-encoded\n",
    "        context = np.ndarray([self.batch_size, self.mem_size])\n",
    "\n",
    "        x.fill(self.init_hid)\n",
    "        for t in xrange(self.mem_size):\n",
    "            time[:,t].fill(t)\n",
    "\n",
    "        if self.show:\n",
    "            from utils import ProgressBar\n",
    "            bar = ProgressBar(label, max=N)\n",
    "\n",
    "        m = self.mem_size\n",
    "        for idx in xrange(N):\n",
    "            if self.show: bar.next()\n",
    "            target.fill(0)\n",
    "            for b in xrange(self.batch_size):\n",
    "                target[b][data[m]] = 1\n",
    "                context[b] = data[m - self.mem_size:m]\n",
    "                m += 1\n",
    "\n",
    "                if m >= len(data):\n",
    "                    m = self.mem_size\n",
    "\n",
    "            loss = self.sess.run([self.loss], feed_dict={self.query: query,\n",
    "                                                         self.time: time,\n",
    "                                                         self.target: target,\n",
    "                                                         self.context: context})\n",
    "            cost += np.sum(loss)\n",
    "\n",
    "        if self.show: bar.finish()\n",
    "        return cost/N/self.batch_size\n",
    "\n",
    "    def run(self, train_data, test_data):\n",
    "        if not self.is_test:\n",
    "            for idx in xrange(self.nepoch):\n",
    "                train_loss = np.sum(self.train(train_data))\n",
    "                test_loss = np.sum(self.test(test_data, label='Validation'))\n",
    "\n",
    "                # Logging\n",
    "                self.log_loss.append([train_loss, test_loss])\n",
    "                self.log_perp.append([math.exp(train_loss), math.exp(test_loss)])\n",
    "\n",
    "                state = {\n",
    "                    'perplexity': math.exp(train_loss),\n",
    "                    'epoch': idx,\n",
    "                    'learning_rate': self.current_lr,\n",
    "                    'valid_perplexity': math.exp(test_loss)\n",
    "                }\n",
    "                print(state)\n",
    "\n",
    "                # Learning rate annealing\n",
    "                if len(self.log_loss) > 1 and self.log_loss[idx][1] > self.log_loss[idx-1][1] * 0.9999:\n",
    "                    self.current_lr = self.current_lr / 1.5\n",
    "                    self.lr.assign(self.current_lr).eval()\n",
    "                if self.current_lr < 1e-5: break\n",
    "\n",
    "                if idx % 10 == 0:\n",
    "                    self.saver.save(self.sess,\n",
    "                                    os.path.join(self.checkpoint_dir, \"MemN2N.model\"),\n",
    "                                    global_step = self.step.astype(int))\n",
    "        else:\n",
    "            self.load()\n",
    "\n",
    "            valid_loss = np.sum(self.test(train_data, label='Validation'))\n",
    "            test_loss = np.sum(self.test(test_data, label='Test'))\n",
    "\n",
    "            state = {\n",
    "                'valid_perplexity': math.exp(valid_loss),\n",
    "                'test_perplexity': math.exp(test_loss)\n",
    "            }\n",
    "            print(state)\n",
    "\n",
    "    def load(self):\n",
    "        print(\" [*] Reading checkpoints...\")\n",
    "        ckpt = tf.train.get_checkpoint_state(self.checkpoint_dir)\n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            self.saver.restore(self.sess, ckpt.model_checkpoint_path)\n",
    "        else:\n",
    "            raise Exception(\" [!] Trest mode but no checkpoint found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def read_data(fname, word2idx, max_words, max_sentences):\n",
    "    # stories[story_idx] = [[sentence1], [sentence2], ..., [sentenceN]]\n",
    "    # questions[question_idx] = {'question': [question], 'answer': [answer], 'options': [[op1],...,[op10],], 'story_index': #, 'sentence_index': #}\n",
    "    stories = dict()\n",
    "    questions = dict()\n",
    "    \n",
    "    if len(word2idx) == 0:\n",
    "        word2idx['<null>'] = 0\n",
    "\n",
    "    if os.path.isfile(fname):\n",
    "        with open(fname,\"r\",encoding = 'utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "    else:\n",
    "        raise Exception(\"[!] Data {file} not found\".format(file=fname))\n",
    "\n",
    "    for line in lines:\n",
    "        #words = line.split(); This fails, we need to split (\\t) to find answer\n",
    "        words = re.split('(\\W)', line)\n",
    "        max_words = max(max_words, len(words))\n",
    "        \n",
    "        # Determine whether the line indicates the start of a new story\n",
    "        if words[0] == '1':\n",
    "            story_idx = len(stories)\n",
    "            sentence_idx = 0\n",
    "            stories[story_idx] = []\n",
    "        \n",
    "        # Determine whether the line is '21'th line, which contains 'XXXX'\n",
    "        if 'XXXXX' in line:\n",
    "            is_question = True\n",
    "            question_idx = len(questions)\n",
    "            questions[question_idx] = {'question': [], 'answer': [], 'options': [], 'story_index': story_idx, 'sentence_index': sentence_idx}\n",
    "        else:\n",
    "            is_question = False\n",
    "            sentence_idx = len(stories[story_idx])\n",
    "        \n",
    "        # Parse and append the words to appropriate dictionary / Expand word2idx dictionary\n",
    "        sentence_list = []\n",
    "        stop_point = 0\n",
    "        for k in range(1, len(words)):\n",
    "            w = words[k].lower()\n",
    "            \n",
    "            if(w=='xxxxx') :\n",
    "                stop_point = 1\n",
    "\n",
    "            # Remove punctuation, or white space, etc.\n",
    "            if ('.' in w) or ('?' in w) or (',' in w) or ('`' in w) or(' ' in w) or ('\\n' in w) or ('!' in w) or (\"'\" in w):\n",
    "                w = w[:-1]\n",
    "            \n",
    "            # Add new word to dictionary\n",
    "            if w not in word2idx:\n",
    "                word2idx[w] = len(word2idx)\n",
    "            \n",
    "            # Append sentence to story dict if not question\n",
    "            if not is_question and not stop_point:\n",
    "                sentence_list.append(w)\n",
    "                \n",
    "                if '.' in words[k]:\n",
    "                    stories[story_idx].append(sentence_list)\n",
    "                    break\n",
    "            # If is question, which contains \"XXXXX\", line 21.\n",
    "            # Append sentence and answer to question dict if question\n",
    "            else:\n",
    "                sentence_list.append(w)\n",
    "                \n",
    "                if '\\t' in words[k]:\n",
    "                    answer = words[k + 1].lower()\n",
    "                    options = []\n",
    "                    # 10 options in total, split by '|'.\n",
    "                    for i in range(1,11):\n",
    "                        options.append(words[k + i*2 + 1])\n",
    "                    if answer not in word2idx:\n",
    "                        word2idx[answer] = len(word2idx)\n",
    "                    \n",
    "                    questions[question_idx]['question'].extend(sentence_list)\n",
    "                    questions[question_idx]['answer'].append(answer)\n",
    "                    questions[question_idx]['options'].append(options)\n",
    "                    break\n",
    "        \n",
    "        # Update max_sentences\n",
    "        max_sentences = max(max_sentences, sentence_idx+1)\n",
    "    \n",
    "    # Convert the words into indices\n",
    "    for idx, context in stories.items():\n",
    "        for i in range(len(context)):\n",
    "            temp = list(map(word2idx.get, context[i]))\n",
    "            context[i] = temp\n",
    "    \n",
    "    for idx, value in questions.items():\n",
    "        temp1 = list(map(word2idx.get, value['question']))\n",
    "        temp2 = list(map(word2idx.get, value['answer']))\n",
    "        \n",
    "        value['question'] = temp1\n",
    "        value['answer'] = temp2\n",
    "    \n",
    "    return stories, questions, max_words, max_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_idx = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a,b,c,d=read_data('training_data/cbtest_NE_train.txt',word_idx,max_words=1344,max_sentences=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1344"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [a,b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "pp = pprint.PrettyPrinter()\n",
    "\n",
    "flags = tf.app.flags\n",
    "\n",
    "flags.DEFINE_integer(\"edim\", 150, \"internal state dimension [150]\")\n",
    "flags.DEFINE_integer(\"lindim\", 75, \"linear part of the state [75]\")\n",
    "flags.DEFINE_integer(\"nhop\", 6, \"number of hops [6]\")\n",
    "flags.DEFINE_integer(\"mem_size\", 100, \"memory size [100]\")\n",
    "flags.DEFINE_integer(\"batch_size\", 128, \"batch size to use during training [128]\")\n",
    "flags.DEFINE_integer(\"nepoch\", 100, \"number of epoch to use during training [100]\")\n",
    "flags.DEFINE_float(\"init_lr\", 0.01, \"initial learning rate [0.01]\")\n",
    "flags.DEFINE_float(\"init_hid\", 0.1, \"initial internal state value [0.1]\")\n",
    "flags.DEFINE_float(\"init_std\", 0.05, \"weight initialization std [0.05]\")\n",
    "flags.DEFINE_float(\"max_grad_norm\", 50, \"clip gradients to this norm [50]\")\n",
    "flags.DEFINE_string(\"data_dir\", \"data\", \"data directory [data]\")\n",
    "flags.DEFINE_string(\"checkpoint_dir\", \"checkpoints\", \"checkpoint directory [checkpoints]\")\n",
    "flags.DEFINE_string(\"data_name\", \"ptb\", \"data set name [ptb]\")\n",
    "flags.DEFINE_boolean(\"is_test\", False, \"True for testing, False for Training [False]\")\n",
    "flags.DEFINE_boolean(\"show\", False, \"print progress [False]\")\n",
    "\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "def main(_):\n",
    "    count = []\n",
    "    word2idx = {}\n",
    "\n",
    "    if not os.path.exists(FLAGS.checkpoint_dir):\n",
    "      os.makedirs(FLAGS.checkpoint_dir)\n",
    "\n",
    "    train_data = read_data('%s/%s.train.txt' % (FLAGS.data_dir, FLAGS.data_name), count, word2idx)\n",
    "    valid_data = read_data('%s/%s.valid.txt' % (FLAGS.data_dir, FLAGS.data_name), count, word2idx)\n",
    "    test_data = read_data('%s/%s.test.txt' % (FLAGS.data_dir, FLAGS.data_name), count, word2idx)\n",
    "\n",
    "    idx2word = dict(zip(word2idx.values(), word2idx.keys()))\n",
    "    FLAGS.nwords = len(word2idx)\n",
    "\n",
    "    pp.pprint(flags.FLAGS.__flags)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        model = MemN2N(FLAGS, sess)\n",
    "        model.build_model()\n",
    "\n",
    "        if FLAGS.is_test:\n",
    "            model.run(valid_data, test_data)\n",
    "        else:\n",
    "            model.run(train_data, valid_data)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    tf.app.run()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
